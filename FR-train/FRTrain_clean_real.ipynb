{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from FRTrain_arch import Generator, DiscriminatorF, DiscriminatorR, weights_init_normal, test_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Autoreload in case that the custom modules are changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is root path of the project\n",
    "root = '/home/hainam/Desktop/KAIST/individual_study_2021/DataExplore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "DiscriminatorR(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "DiscriminatorF(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Generator())\n",
    "print(DiscriminatorR())\n",
    "print(DiscriminatorF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process real data from Adult Cencus dataset ( without crowdsourcing validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = Namespace(num_train=2000, num_val1=200, num_val2=500, num_test=1000, in_feature=11)\n",
    "\n",
    "# num_train = opt.num_train\n",
    "# num_val1 = opt.num_val1\n",
    "# num_val2 = opt.num_val2\n",
    "# num_test = opt.num_test\n",
    "\n",
    "# XS = np.load(os.path.join(root,'data/adultcensus_data/XS_adult.npy'), allow_pickle=True) # Input features\n",
    "# y = np.load(os.path.join(root, 'data/adultcensus_data/y_adult.npy'), allow_pickle=True) # Labels\n",
    "# s1 = np.load(os.path.join(root,'data/adultcensus_data/s_adult.npy'), allow_pickle=True) # Sensitive features\n",
    "# s1 = s1.astype(float)\n",
    "\n",
    "# XS = torch.FloatTensor(XS)\n",
    "# y = torch.FloatTensor(y)\n",
    "# s1 = torch.FloatTensor(s1)\n",
    "\n",
    "# XS_train = XS[:num_train - num_val1]\n",
    "# y_train = y[:num_train - num_val1]\n",
    "# s1_train = s1[:num_train - num_val1]\n",
    "\n",
    "# # 10% val set size\n",
    "# XS_val = XS[num_train: num_train + num_val1]\n",
    "# y_val = y[num_train: num_train + num_val1]\n",
    "# s1_val = s1[num_train: num_train + num_val1]\n",
    "\n",
    "# XS_test = XS[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "# y_test = y[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "# s1_test = s1[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process real data from Adult Cencus dataset ( with crowdsourcing validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# opt = Namespace(num_train=5000, num_val=500, num_test=1000, in_feature = 11)\n",
    "\n",
    "# num_train = opt.num_train\n",
    "# num_val = opt.num_val\n",
    "# num_test = opt.num_test\n",
    "\n",
    "# XS = np.load(os.path.join(root,'data/adultcensus_data/XS_train_adult.npy'), allow_pickle=True) # Input features\n",
    "# y = np.load(os.path.join(root,'data/adultcensus_data/y_train_adult.npy'), allow_pickle=True) # Labels\n",
    "# s1 = np.load(os.path.join(root,'data/adultcensus_data/s_train_adult.npy'), allow_pickle=True) # Sensitive features\n",
    "# s1 = s1.astype(float)\n",
    "\n",
    "# XS = torch.FloatTensor(XS)\n",
    "# y = torch.FloatTensor(y)\n",
    "# s1 = torch.FloatTensor(s1)\n",
    "\n",
    "# XS_train = XS[:num_train]\n",
    "# y_train = y[:num_train] \n",
    "# s1_train = s1[:num_train]\n",
    "\n",
    "\n",
    "# XS_test = XS[num_train: num_train + num_test]\n",
    "# y_test = y[num_train: num_train + num_test]\n",
    "# s1_test = s1[num_train: num_train + num_test]\n",
    "\n",
    "# # 10% validation set constructing from crowdsourcing data\n",
    "# XS_val = np.load(os.path.join(root,'data/adultcensus_data/XS_val_adult.npy'), allow_pickle=True)\n",
    "# y_val = np.load(os.path.join(root, 'data/adultcensus_data/y_val_adult.npy'), allow_pickle=True)\n",
    "# s1_val = np.load(os.path.join(root,'data/adultcensus_data/s_val_adult.npy'), allow_pickle=True)\n",
    "# s1_val = s1_val.astype(float)\n",
    "\n",
    "\n",
    "# XS_val = torch.FloatTensor(XS_val)\n",
    "# y_val = torch.FloatTensor(y_val)\n",
    "# s1_val = torch.FloatTensor(s1_val)\n",
    "\n",
    "# XS_val = XS_val[:num_val]\n",
    "# y_val = y_val[:num_val]\n",
    "# s1 = s1[:num_val]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process real data from COMPAS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace(num_train=2000, num_val1=200, num_val2=500, num_test=1000, in_feature=7)\n",
    "\n",
    "num_train = opt.num_train\n",
    "num_val1 = opt.num_val1\n",
    "num_val2 = opt.num_val2\n",
    "num_test = opt.num_test\n",
    "\n",
    "XS = np.load( os.path.join(root, 'data/compas_data/XS_compas.npy'), allow_pickle=True ) # Input features\n",
    "y = np.load(os.path.join(root, 'data/compas_data/y_compas.npy'), allow_pickle=True) # Labels\n",
    "s1 = np.load(os.path.join(root, 'data/compas_data/s_compas.npy'), allow_pickle=True) # Sensitive features\n",
    "s1 = s1.astype(float)\n",
    "\n",
    "XS = torch.FloatTensor(XS)\n",
    "y = torch.FloatTensor(y)\n",
    "s1 = torch.FloatTensor(s1)\n",
    "\n",
    "XS_train = XS[:num_train - num_val1]\n",
    "y_train = y[:num_train - num_val1]\n",
    "s1_train = s1[:num_train - num_val1]\n",
    "\n",
    "# 10% val set size\n",
    "XS_val = XS[num_train - num_val1: num_train]\n",
    "y_val = y[num_train - num_val1: num_train]\n",
    "s1_val = s1[num_train - num_val1: num_train]\n",
    "\n",
    "XS_test = XS[num_train : num_train + num_test]\n",
    "y_test = y[num_train : num_train + num_test]\n",
    "s1_test = s1[num_train : num_train + num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XS train torch.Size([1800, 7])\n",
      "y train torch.Size([1800])\n",
      "s1 train torch.Size([1800])\n",
      "------------------------------\n",
      "XS val torch.Size([200, 7])\n",
      "y val torch.Size([200])\n",
      "s1 val torch.Size([200])\n",
      "------------------------------\n",
      "XS test torch.Size([1000, 7])\n",
      "y test torch.Size([1000])\n",
      "s1 test torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"XS train\", XS_train.shape)\n",
    "print(\"y train\", y_train.shape)\n",
    "print(\"s1 train\", s1_train.shape)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "\n",
    "print(\"XS val\", XS_val.shape)\n",
    "print(\"y val\", y_val.shape)\n",
    "print(\"s1 val\", s1_val.shape)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "\n",
    "\n",
    "print(\"XS test\", XS_test.shape)\n",
    "print(\"y test\", y_test.shape)\n",
    "print(\"s1 test\", s1_test.shape)\n",
    "\n",
    "# print(XS_train)\n",
    "# print(y_train)\n",
    "# print(s1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Number of Data -------------------------\n",
      "Train data : 1800, Validation data : 200, Test data : 1000 \n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------- Number of Data -------------------------\" )\n",
    "print(\n",
    "    \"Train data : %d, Validation data : %d, Test data : %d \"\n",
    "    % (len(y_train), len(y_val), len(y_test))\n",
    ")       \n",
    "print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f, lambda_r, seed):\n",
    "    \"\"\"\n",
    "      Trains FR-Train by using the classes in FRTrain_arch.py.\n",
    "      \n",
    "      Args:\n",
    "        train_tensors: Training data.\n",
    "        val_tensors: Clean validation data.\n",
    "        test_tensors: Test data.\n",
    "        train_opt: Options for the training. It currently contains size of validation set, \n",
    "                number of epochs, generator/discriminator update ratio, and learning rates.\n",
    "        lambda_f: The tuning knob for L_2 (ref: FR-Train paper, Section 3.3).\n",
    "        lambda_r: The tuning knob for L_3 (ref: FR-Train paper, Section 3.3).\n",
    "        seed: An integer value for specifying torch random seed.\n",
    "        \n",
    "      Returns:\n",
    "        Information about the tuning knobs (lambda_f, lambda_r),\n",
    "        the test accuracy of the trained model, and disparate impact of the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    XS_train = train_tensors.XS_train\n",
    "    y_train = train_tensors.y_train\n",
    "    s1_train = train_tensors.s1_train\n",
    "    \n",
    "    XS_val = val_tensors.XS_val\n",
    "    y_val = val_tensors.y_val\n",
    "    s1_val = val_tensors.s1_val\n",
    "    \n",
    "    XS_test = test_tensors.XS_test\n",
    "    y_test = test_tensors.y_test\n",
    "    s1_test = test_tensors.s1_test\n",
    "    \n",
    "    # Saves return values here\n",
    "    test_result = []\n",
    "    \n",
    "    val = train_opt.val # Number of data points in validation set\n",
    "    k = train_opt.k     # Updates ratio of generator and discriminator (1:k training).\n",
    "    n_epochs = train_opt.n_epochs  # Number of training epoch\n",
    "    \n",
    "    # Changes the input validation data to an appropriate shape for the training\n",
    "    XSY_val = torch.cat([XS_val, y_val.reshape((y_val.shape[0], 1))], dim=1)  \n",
    "\n",
    "    # The loss values of each component will be saved in the following lists. \n",
    "    # We can draw epoch-loss graph by the following lists, if necessary.\n",
    "    g_losses =[]\n",
    "    d_f_losses = []\n",
    "    d_r_losses = []\n",
    "    clean_test_result = []\n",
    "\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "    # Initializes generator and discriminator\n",
    "    generator = Generator(in_feat=opt.in_feature)\n",
    "    discriminator_F = DiscriminatorF()\n",
    "\n",
    "    discriminator_R = DiscriminatorR(in_feat = opt.in_feature + 1)\n",
    "\n",
    "    # Initializes weights\n",
    "    torch.manual_seed(seed)\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator_F.apply(weights_init_normal)\n",
    "    discriminator_R.apply(weights_init_normal)\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=train_opt.lr_g)\n",
    "    optimizer_D_F = torch.optim.SGD(discriminator_F.parameters(), lr=train_opt.lr_f)\n",
    "    optimizer_D_R = torch.optim.SGD(discriminator_R.parameters(), lr=train_opt.lr_r)\n",
    "\n",
    "    XSY_val_data = XSY_val[:val]\n",
    "\n",
    "    train_len = XS_train.shape[0]\n",
    "    val_len = XSY_val.shape[0]\n",
    "\n",
    "    # Ground truths using in Disriminator_R\n",
    "    Tensor = torch.FloatTensor\n",
    "    valid = Variable(Tensor(train_len, 1).fill_(1.0), requires_grad=False)\n",
    "    generated = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    fake = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    clean = Variable(Tensor(val_len, 1).fill_(1.0), requires_grad=False)\n",
    "    \n",
    "    r_weight = torch.ones_like(y_train, requires_grad=False).float()\n",
    "    r_ones = torch.ones_like(y_train, requires_grad=False).float()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # -------------------\n",
    "        #  Forwards Generator\n",
    "        # -------------------\n",
    "        if epoch % k == 0 or epoch < 500:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "        gen_y = generator(XS_train)\n",
    "        gen_data = torch.cat([XS_train, gen_y.detach().reshape((gen_y.shape[0], 1))], dim=1)\n",
    "        \n",
    "        # -----------------------------\n",
    "        #  Trains Fairness Discriminator\n",
    "        # -----------------------------\n",
    "\n",
    "        optimizer_D_F.zero_grad()\n",
    "\n",
    "        # Discriminator_F tries to distinguish the sensitive groups by using the output of the generator.\n",
    "        d_f_loss= bce_loss(discriminator_F(gen_y.detach()), s1_train.unsqueeze(1))\n",
    "        d_f_loss.backward()\n",
    "        d_f_losses.append(d_f_loss)\n",
    "        optimizer_D_F.step()\n",
    "            \n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Trains Robustness Discriminator\n",
    "        # ---------------------------------\n",
    "        optimizer_D_R.zero_grad()\n",
    "\n",
    "        # Discriminator_R tries to distinguish whether the input is from the validation data or the generated data from generator.\n",
    "        clean_loss = bce_loss(discriminator_R(XSY_val_data), clean)\n",
    "        poison_loss = bce_loss(discriminator_R(gen_data.detach()), fake)\n",
    "        d_r_loss = 0.5 * (clean_loss + poison_loss)\n",
    "\n",
    "        d_r_loss.backward()\n",
    "        d_r_losses.append(d_r_loss)\n",
    "        optimizer_D_R.step()\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Updates Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminators\n",
    "        if epoch < 500 :\n",
    "            g_loss = bce_loss((F.tanh(gen_y)+1)/2, (y_train.unsqueeze(1)+1)/2)\n",
    "            g_loss.backward()\n",
    "            g_losses.append(g_loss)\n",
    "            optimizer_G.step()\n",
    "    \n",
    "        elif epoch % k == 0:\n",
    "            r_decision = discriminator_R(gen_data)\n",
    "            r_gen = bce_loss(r_decision, generated)\n",
    "            \n",
    "            # ------------------------------\n",
    "            #  Re-weights using output of D_R\n",
    "            # ------------------------------\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss_ratio = (g_losses[-1]/d_r_losses[-1]).detach()\n",
    "                a = 1/(1+torch.exp(-(loss_ratio-3)))\n",
    "                b = 1-a\n",
    "                r_weight_tmp = r_decision.detach().squeeze()\n",
    "                r_weight = a * r_weight_tmp + b * r_ones\n",
    "\n",
    "            f_cost = F.binary_cross_entropy(discriminator_F(gen_y), s1_train.unsqueeze(1), reduction=\"none\").squeeze()\n",
    "            g_cost = F.binary_cross_entropy_with_logits(gen_y.squeeze(), (y_train.squeeze()+1)/2, reduction=\"none\").squeeze()\n",
    "\n",
    "            f_gen = torch.mean(f_cost*r_weight)\n",
    "            g_loss = (1-lambda_f-lambda_r) * torch.mean(g_cost*r_weight) -  lambda_r * r_gen - lambda_f * f_gen\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print(\n",
    "                \"[Lambda_f: %1f] [Epoch %d/%d] [D_F loss: %f] [D_R loss: %f] [G loss: %f]\"\n",
    "                % (lambda_f, epoch, n_epochs, d_f_losses[-1], d_r_losses[-1], g_losses[-1])\n",
    "            )\n",
    "\n",
    "#     torch.save(generator.state_dict(), './FR-Train_on_clean_synthetic.pth')\n",
    "    tmp = test_model(generator, XS_test, y_test, s1_test)\n",
    "    test_result.append([lambda_f, lambda_r, tmp[0].item(), tmp[1]])\n",
    "\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda_f: 0.200000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.200000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.200000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.200000] [Epoch 600/4000] [D_F loss: 0.639621] [D_R loss: 0.723444] [G loss: 0.205393]\n",
      "[Lambda_f: 0.200000] [Epoch 800/4000] [D_F loss: 0.637316] [D_R loss: 0.720628] [G loss: 0.202512]\n",
      "[Lambda_f: 0.200000] [Epoch 1000/4000] [D_F loss: 0.636190] [D_R loss: 0.717107] [G loss: 0.202436]\n",
      "[Lambda_f: 0.200000] [Epoch 1200/4000] [D_F loss: 0.635798] [D_R loss: 0.714069] [G loss: 0.202894]\n",
      "[Lambda_f: 0.200000] [Epoch 1400/4000] [D_F loss: 0.635673] [D_R loss: 0.711376] [G loss: 0.203696]\n",
      "[Lambda_f: 0.200000] [Epoch 1600/4000] [D_F loss: 0.635637] [D_R loss: 0.708965] [G loss: 0.204414]\n",
      "[Lambda_f: 0.200000] [Epoch 1800/4000] [D_F loss: 0.635633] [D_R loss: 0.706793] [G loss: 0.205112]\n",
      "[Lambda_f: 0.200000] [Epoch 2000/4000] [D_F loss: 0.635635] [D_R loss: 0.704810] [G loss: 0.205852]\n",
      "[Lambda_f: 0.200000] [Epoch 2200/4000] [D_F loss: 0.635638] [D_R loss: 0.702989] [G loss: 0.206436]\n",
      "[Lambda_f: 0.200000] [Epoch 2400/4000] [D_F loss: 0.635642] [D_R loss: 0.701316] [G loss: 0.206967]\n",
      "[Lambda_f: 0.200000] [Epoch 2600/4000] [D_F loss: 0.635644] [D_R loss: 0.699747] [G loss: 0.207522]\n",
      "[Lambda_f: 0.200000] [Epoch 2800/4000] [D_F loss: 0.635645] [D_R loss: 0.698288] [G loss: 0.207956]\n",
      "[Lambda_f: 0.200000] [Epoch 3000/4000] [D_F loss: 0.635647] [D_R loss: 0.696912] [G loss: 0.208353]\n",
      "[Lambda_f: 0.200000] [Epoch 3200/4000] [D_F loss: 0.635648] [D_R loss: 0.695610] [G loss: 0.208768]\n",
      "[Lambda_f: 0.200000] [Epoch 3400/4000] [D_F loss: 0.635648] [D_R loss: 0.694376] [G loss: 0.209093]\n",
      "[Lambda_f: 0.200000] [Epoch 3600/4000] [D_F loss: 0.635649] [D_R loss: 0.693194] [G loss: 0.209393]\n",
      "[Lambda_f: 0.200000] [Epoch 3800/4000] [D_F loss: 0.635649] [D_R loss: 0.692063] [G loss: 0.209710]\n",
      "Test accuracy: 0.6520000100135803\n",
      "P(y_hat=1 | z=0) = 0.320, P(y_hat=1 | z=1) = 0.300\n",
      "P(y_hat=1 | y=1, z=0) = 0.518, P(y_hat=1 | y=1, z=1) = 0.432\n",
      "Disparate Impact ratio = 0.938\n",
      "[Lambda_f: 0.300000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.300000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.300000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.300000] [Epoch 600/4000] [D_F loss: 0.643612] [D_R loss: 0.724295] [G loss: 0.085160]\n",
      "[Lambda_f: 0.300000] [Epoch 800/4000] [D_F loss: 0.638354] [D_R loss: 0.721016] [G loss: 0.083388]\n",
      "[Lambda_f: 0.300000] [Epoch 1000/4000] [D_F loss: 0.637147] [D_R loss: 0.717432] [G loss: 0.083490]\n",
      "[Lambda_f: 0.300000] [Epoch 1200/4000] [D_F loss: 0.636805] [D_R loss: 0.714398] [G loss: 0.084029]\n",
      "[Lambda_f: 0.300000] [Epoch 1400/4000] [D_F loss: 0.636710] [D_R loss: 0.711712] [G loss: 0.084801]\n",
      "[Lambda_f: 0.300000] [Epoch 1600/4000] [D_F loss: 0.636684] [D_R loss: 0.709297] [G loss: 0.085558]\n",
      "[Lambda_f: 0.300000] [Epoch 1800/4000] [D_F loss: 0.636680] [D_R loss: 0.707116] [G loss: 0.086295]\n",
      "[Lambda_f: 0.300000] [Epoch 2000/4000] [D_F loss: 0.636680] [D_R loss: 0.705121] [G loss: 0.087026]\n",
      "[Lambda_f: 0.300000] [Epoch 2200/4000] [D_F loss: 0.636681] [D_R loss: 0.703290] [G loss: 0.087655]\n",
      "[Lambda_f: 0.300000] [Epoch 2400/4000] [D_F loss: 0.636683] [D_R loss: 0.701609] [G loss: 0.088228]\n",
      "[Lambda_f: 0.300000] [Epoch 2600/4000] [D_F loss: 0.636683] [D_R loss: 0.700038] [G loss: 0.088784]\n",
      "[Lambda_f: 0.300000] [Epoch 2800/4000] [D_F loss: 0.636683] [D_R loss: 0.698574] [G loss: 0.089257]\n",
      "[Lambda_f: 0.300000] [Epoch 3000/4000] [D_F loss: 0.636684] [D_R loss: 0.697196] [G loss: 0.089689]\n",
      "[Lambda_f: 0.300000] [Epoch 3200/4000] [D_F loss: 0.636684] [D_R loss: 0.695889] [G loss: 0.090106]\n",
      "[Lambda_f: 0.300000] [Epoch 3400/4000] [D_F loss: 0.636684] [D_R loss: 0.694651] [G loss: 0.090462]\n",
      "[Lambda_f: 0.300000] [Epoch 3600/4000] [D_F loss: 0.636684] [D_R loss: 0.693467] [G loss: 0.090788]\n",
      "[Lambda_f: 0.300000] [Epoch 3800/4000] [D_F loss: 0.636684] [D_R loss: 0.692337] [G loss: 0.091107]\n",
      "Test accuracy: 0.6470000147819519\n",
      "P(y_hat=1 | z=0) = 0.346, P(y_hat=1 | z=1) = 0.294\n",
      "P(y_hat=1 | y=1, z=0) = 0.540, P(y_hat=1 | y=1, z=1) = 0.423\n",
      "Disparate Impact ratio = 0.850\n",
      "[Lambda_f: 0.400000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.400000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.400000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.400000] [Epoch 600/4000] [D_F loss: 0.647106] [D_R loss: 0.725282] [G loss: -0.036678]\n",
      "[Lambda_f: 0.400000] [Epoch 800/4000] [D_F loss: 0.638167] [D_R loss: 0.720944] [G loss: -0.036898]\n",
      "[Lambda_f: 0.400000] [Epoch 1000/4000] [D_F loss: 0.637585] [D_R loss: 0.717674] [G loss: -0.036618]\n",
      "[Lambda_f: 0.400000] [Epoch 1200/4000] [D_F loss: 0.637291] [D_R loss: 0.714660] [G loss: -0.035996]\n",
      "[Lambda_f: 0.400000] [Epoch 1400/4000] [D_F loss: 0.637217] [D_R loss: 0.711988] [G loss: -0.035248]\n",
      "[Lambda_f: 0.400000] [Epoch 1600/4000] [D_F loss: 0.637191] [D_R loss: 0.709571] [G loss: -0.034477]\n",
      "[Lambda_f: 0.400000] [Epoch 1800/4000] [D_F loss: 0.637184] [D_R loss: 0.707379] [G loss: -0.033724]\n",
      "[Lambda_f: 0.400000] [Epoch 2000/4000] [D_F loss: 0.637183] [D_R loss: 0.705373] [G loss: -0.033009]\n",
      "[Lambda_f: 0.400000] [Epoch 2200/4000] [D_F loss: 0.637182] [D_R loss: 0.703531] [G loss: -0.032353]\n",
      "[Lambda_f: 0.400000] [Epoch 2400/4000] [D_F loss: 0.637183] [D_R loss: 0.701839] [G loss: -0.031755]\n",
      "[Lambda_f: 0.400000] [Epoch 2600/4000] [D_F loss: 0.637183] [D_R loss: 0.700267] [G loss: -0.031202]\n",
      "[Lambda_f: 0.400000] [Epoch 2800/4000] [D_F loss: 0.637182] [D_R loss: 0.698797] [G loss: -0.030703]\n",
      "[Lambda_f: 0.400000] [Epoch 3000/4000] [D_F loss: 0.637182] [D_R loss: 0.697417] [G loss: -0.030248]\n",
      "[Lambda_f: 0.400000] [Epoch 3200/4000] [D_F loss: 0.637182] [D_R loss: 0.696106] [G loss: -0.029829]\n",
      "[Lambda_f: 0.400000] [Epoch 3400/4000] [D_F loss: 0.637181] [D_R loss: 0.694864] [G loss: -0.029452]\n",
      "[Lambda_f: 0.400000] [Epoch 3600/4000] [D_F loss: 0.637181] [D_R loss: 0.693679] [G loss: -0.029107]\n",
      "[Lambda_f: 0.400000] [Epoch 3800/4000] [D_F loss: 0.637181] [D_R loss: 0.692549] [G loss: -0.028787]\n",
      "Test accuracy: 0.6449999809265137\n",
      "P(y_hat=1 | z=0) = 0.354, P(y_hat=1 | z=1) = 0.289\n",
      "P(y_hat=1 | y=1, z=0) = 0.547, P(y_hat=1 | y=1, z=1) = 0.417\n",
      "Disparate Impact ratio = 0.816\n",
      "[Lambda_f: 0.500000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.500000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.500000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.500000] [Epoch 600/4000] [D_F loss: 0.649870] [D_R loss: 0.726383] [G loss: -0.159943]\n",
      "[Lambda_f: 0.500000] [Epoch 800/4000] [D_F loss: 0.638087] [D_R loss: 0.720310] [G loss: -0.158270]\n",
      "[Lambda_f: 0.500000] [Epoch 1000/4000] [D_F loss: 0.638127] [D_R loss: 0.718108] [G loss: -0.157739]\n",
      "[Lambda_f: 0.500000] [Epoch 1200/4000] [D_F loss: 0.637478] [D_R loss: 0.714842] [G loss: -0.156943]\n",
      "[Lambda_f: 0.500000] [Epoch 1400/4000] [D_F loss: 0.637516] [D_R loss: 0.712273] [G loss: -0.156217]\n",
      "[Lambda_f: 0.500000] [Epoch 1600/4000] [D_F loss: 0.637437] [D_R loss: 0.709814] [G loss: -0.155434]\n",
      "[Lambda_f: 0.500000] [Epoch 1800/4000] [D_F loss: 0.637444] [D_R loss: 0.707627] [G loss: -0.154686]\n",
      "[Lambda_f: 0.500000] [Epoch 2000/4000] [D_F loss: 0.637438] [D_R loss: 0.705609] [G loss: -0.153992]\n",
      "[Lambda_f: 0.500000] [Epoch 2200/4000] [D_F loss: 0.637437] [D_R loss: 0.703755] [G loss: -0.153330]\n",
      "[Lambda_f: 0.500000] [Epoch 2400/4000] [D_F loss: 0.637437] [D_R loss: 0.702051] [G loss: -0.152721]\n",
      "[Lambda_f: 0.500000] [Epoch 2600/4000] [D_F loss: 0.637437] [D_R loss: 0.700474] [G loss: -0.152175]\n",
      "[Lambda_f: 0.500000] [Epoch 2800/4000] [D_F loss: 0.637436] [D_R loss: 0.699001] [G loss: -0.151662]\n",
      "[Lambda_f: 0.500000] [Epoch 3000/4000] [D_F loss: 0.637435] [D_R loss: 0.697615] [G loss: -0.151195]\n",
      "[Lambda_f: 0.500000] [Epoch 3200/4000] [D_F loss: 0.637435] [D_R loss: 0.696301] [G loss: -0.150775]\n",
      "[Lambda_f: 0.500000] [Epoch 3400/4000] [D_F loss: 0.637434] [D_R loss: 0.695053] [G loss: -0.150384]\n",
      "[Lambda_f: 0.500000] [Epoch 3600/4000] [D_F loss: 0.637434] [D_R loss: 0.693867] [G loss: -0.150028]\n",
      "[Lambda_f: 0.500000] [Epoch 3800/4000] [D_F loss: 0.637434] [D_R loss: 0.692733] [G loss: -0.149706]\n",
      "Test accuracy: 0.6439999938011169\n",
      "P(y_hat=1 | z=0) = 0.366, P(y_hat=1 | z=1) = 0.288\n",
      "P(y_hat=1 | y=1, z=0) = 0.555, P(y_hat=1 | y=1, z=1) = 0.417\n",
      "Disparate Impact ratio = 0.787\n",
      "[Lambda_f: 0.600000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.600000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.600000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.600000] [Epoch 600/4000] [D_F loss: 0.651779] [D_R loss: 0.727592] [G loss: -0.284539]\n",
      "[Lambda_f: 0.600000] [Epoch 800/4000] [D_F loss: 0.639768] [D_R loss: 0.719221] [G loss: -0.281215]\n",
      "[Lambda_f: 0.600000] [Epoch 1000/4000] [D_F loss: 0.638951] [D_R loss: 0.718736] [G loss: -0.279925]\n",
      "[Lambda_f: 0.600000] [Epoch 1200/4000] [D_F loss: 0.637725] [D_R loss: 0.715070] [G loss: -0.278695]\n",
      "[Lambda_f: 0.600000] [Epoch 1400/4000] [D_F loss: 0.637570] [D_R loss: 0.712519] [G loss: -0.277899]\n",
      "[Lambda_f: 0.600000] [Epoch 1600/4000] [D_F loss: 0.637612] [D_R loss: 0.710115] [G loss: -0.277128]\n",
      "[Lambda_f: 0.600000] [Epoch 1800/4000] [D_F loss: 0.637557] [D_R loss: 0.707891] [G loss: -0.276381]\n",
      "[Lambda_f: 0.600000] [Epoch 2000/4000] [D_F loss: 0.637564] [D_R loss: 0.705863] [G loss: -0.275711]\n",
      "[Lambda_f: 0.600000] [Epoch 2200/4000] [D_F loss: 0.637582] [D_R loss: 0.704027] [G loss: -0.275061]\n",
      "[Lambda_f: 0.600000] [Epoch 2400/4000] [D_F loss: 0.637555] [D_R loss: 0.702279] [G loss: -0.274452]\n",
      "[Lambda_f: 0.600000] [Epoch 2600/4000] [D_F loss: 0.637577] [D_R loss: 0.700716] [G loss: -0.273923]\n",
      "[Lambda_f: 0.600000] [Epoch 2800/4000] [D_F loss: 0.637561] [D_R loss: 0.699220] [G loss: -0.273407]\n",
      "[Lambda_f: 0.600000] [Epoch 3000/4000] [D_F loss: 0.637568] [D_R loss: 0.697839] [G loss: -0.272939]\n",
      "[Lambda_f: 0.600000] [Epoch 3200/4000] [D_F loss: 0.637565] [D_R loss: 0.696519] [G loss: -0.272522]\n",
      "[Lambda_f: 0.600000] [Epoch 3400/4000] [D_F loss: 0.637564] [D_R loss: 0.695264] [G loss: -0.272124]\n",
      "[Lambda_f: 0.600000] [Epoch 3600/4000] [D_F loss: 0.637565] [D_R loss: 0.694072] [G loss: -0.271762]\n",
      "[Lambda_f: 0.600000] [Epoch 3800/4000] [D_F loss: 0.637564] [D_R loss: 0.692932] [G loss: -0.271441]\n",
      "Test accuracy: 0.6439999938011169\n",
      "P(y_hat=1 | z=0) = 0.369, P(y_hat=1 | z=1) = 0.286\n",
      "P(y_hat=1 | y=1, z=0) = 0.562, P(y_hat=1 | y=1, z=1) = 0.414\n",
      "Disparate Impact ratio = 0.776\n",
      "[Lambda_f: 0.700000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.700000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.700000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.700000] [Epoch 600/4000] [D_F loss: 0.652852] [D_R loss: 0.728939] [G loss: -0.410408]\n",
      "[Lambda_f: 0.700000] [Epoch 800/4000] [D_F loss: 0.644102] [D_R loss: 0.717897] [G loss: -0.406526]\n",
      "[Lambda_f: 0.700000] [Epoch 1000/4000] [D_F loss: 0.637683] [D_R loss: 0.718416] [G loss: -0.401609]\n",
      "[Lambda_f: 0.700000] [Epoch 1200/4000] [D_F loss: 0.635759] [D_R loss: 0.717555] [G loss: -0.398891]\n",
      "[Lambda_f: 0.700000] [Epoch 1400/4000] [D_F loss: 0.635497] [D_R loss: 0.710577] [G loss: -0.398692]\n",
      "[Lambda_f: 0.700000] [Epoch 1600/4000] [D_F loss: 0.639691] [D_R loss: 0.712480] [G loss: -0.400256]\n",
      "[Lambda_f: 0.700000] [Epoch 1800/4000] [D_F loss: 0.638028] [D_R loss: 0.707323] [G loss: -0.398969]\n",
      "[Lambda_f: 0.700000] [Epoch 2000/4000] [D_F loss: 0.637708] [D_R loss: 0.706347] [G loss: -0.398033]\n",
      "[Lambda_f: 0.700000] [Epoch 2200/4000] [D_F loss: 0.637636] [D_R loss: 0.704715] [G loss: -0.397223]\n",
      "[Lambda_f: 0.700000] [Epoch 2400/4000] [D_F loss: 0.637055] [D_R loss: 0.702038] [G loss: -0.396456]\n",
      "[Lambda_f: 0.700000] [Epoch 2600/4000] [D_F loss: 0.637881] [D_R loss: 0.701593] [G loss: -0.396264]\n",
      "[Lambda_f: 0.700000] [Epoch 2800/4000] [D_F loss: 0.637334] [D_R loss: 0.699040] [G loss: -0.395615]\n",
      "[Lambda_f: 0.700000] [Epoch 3000/4000] [D_F loss: 0.637866] [D_R loss: 0.698506] [G loss: -0.395360]\n",
      "[Lambda_f: 0.700000] [Epoch 3200/4000] [D_F loss: 0.637484] [D_R loss: 0.696525] [G loss: -0.394834]\n",
      "[Lambda_f: 0.700000] [Epoch 3400/4000] [D_F loss: 0.637770] [D_R loss: 0.695766] [G loss: -0.394538]\n",
      "[Lambda_f: 0.700000] [Epoch 3600/4000] [D_F loss: 0.637543] [D_R loss: 0.694185] [G loss: -0.394100]\n",
      "[Lambda_f: 0.700000] [Epoch 3800/4000] [D_F loss: 0.637711] [D_R loss: 0.693326] [G loss: -0.393838]\n",
      "Test accuracy: 0.6449999809265137\n",
      "P(y_hat=1 | z=0) = 0.369, P(y_hat=1 | z=1) = 0.288\n",
      "P(y_hat=1 | y=1, z=0) = 0.562, P(y_hat=1 | y=1, z=1) = 0.417\n",
      "Disparate Impact ratio = 0.781\n",
      "[Lambda_f: 0.800000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.800000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.800000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.800000] [Epoch 600/4000] [D_F loss: 0.653207] [D_R loss: 0.730597] [G loss: -0.537611]\n",
      "[Lambda_f: 0.800000] [Epoch 800/4000] [D_F loss: 0.650755] [D_R loss: 0.716465] [G loss: -0.534653]\n",
      "[Lambda_f: 0.800000] [Epoch 1000/4000] [D_F loss: 0.626351] [D_R loss: 0.715638] [G loss: -0.515037]\n",
      "[Lambda_f: 0.800000] [Epoch 1200/4000] [D_F loss: 0.634048] [D_R loss: 0.723653] [G loss: -0.519456]\n",
      "[Lambda_f: 0.800000] [Epoch 1400/4000] [D_F loss: 0.649205] [D_R loss: 0.709665] [G loss: -0.530757]\n",
      "[Lambda_f: 0.800000] [Epoch 1600/4000] [D_F loss: 0.628318] [D_R loss: 0.708884] [G loss: -0.514286]\n",
      "[Lambda_f: 0.800000] [Epoch 1800/4000] [D_F loss: 0.639881] [D_R loss: 0.714842] [G loss: -0.522213]\n",
      "[Lambda_f: 0.800000] [Epoch 2000/4000] [D_F loss: 0.642368] [D_R loss: 0.705311] [G loss: -0.523752]\n",
      "[Lambda_f: 0.800000] [Epoch 2200/4000] [D_F loss: 0.630696] [D_R loss: 0.702249] [G loss: -0.514096]\n",
      "[Lambda_f: 0.800000] [Epoch 2400/4000] [D_F loss: 0.641764] [D_R loss: 0.706888] [G loss: -0.522263]\n",
      "[Lambda_f: 0.800000] [Epoch 2600/4000] [D_F loss: 0.638154] [D_R loss: 0.701283] [G loss: -0.519007]\n",
      "[Lambda_f: 0.800000] [Epoch 2800/4000] [D_F loss: 0.634081] [D_R loss: 0.697612] [G loss: -0.515287]\n",
      "[Lambda_f: 0.800000] [Epoch 3000/4000] [D_F loss: 0.640334] [D_R loss: 0.700802] [G loss: -0.519889]\n",
      "[Lambda_f: 0.800000] [Epoch 3200/4000] [D_F loss: 0.637590] [D_R loss: 0.697278] [G loss: -0.517365]\n",
      "[Lambda_f: 0.800000] [Epoch 3400/4000] [D_F loss: 0.635718] [D_R loss: 0.694225] [G loss: -0.515499]\n",
      "[Lambda_f: 0.800000] [Epoch 3600/4000] [D_F loss: 0.639003] [D_R loss: 0.696300] [G loss: -0.517779]\n",
      "[Lambda_f: 0.800000] [Epoch 3800/4000] [D_F loss: 0.637958] [D_R loss: 0.693335] [G loss: -0.516689]\n",
      "Test accuracy: 0.6539999842643738\n",
      "P(y_hat=1 | z=0) = 0.314, P(y_hat=1 | z=1) = 0.300\n",
      "P(y_hat=1 | y=1, z=0) = 0.504, P(y_hat=1 | y=1, z=1) = 0.438\n",
      "Disparate Impact ratio = 0.955\n",
      "[Lambda_f: 0.850000] [Epoch 0/4000] [D_F loss: 0.694610] [D_R loss: 0.738519] [G loss: 0.834932]\n",
      "[Lambda_f: 0.850000] [Epoch 200/4000] [D_F loss: 0.656509] [D_R loss: 0.729279] [G loss: 0.595149]\n",
      "[Lambda_f: 0.850000] [Epoch 400/4000] [D_F loss: 0.642267] [D_R loss: 0.725206] [G loss: 0.594965]\n",
      "[Lambda_f: 0.850000] [Epoch 600/4000] [D_F loss: 0.653152] [D_R loss: 0.731624] [G loss: -0.601895]\n",
      "[Lambda_f: 0.850000] [Epoch 800/4000] [D_F loss: 0.654501] [D_R loss: 0.715995] [G loss: -0.600214]\n",
      "[Lambda_f: 0.850000] [Epoch 1000/4000] [D_F loss: 0.612789] [D_R loss: 0.712154] [G loss: -0.563642]\n",
      "[Lambda_f: 0.850000] [Epoch 1200/4000] [D_F loss: 0.649001] [D_R loss: 0.726201] [G loss: -0.594946]\n",
      "[Lambda_f: 0.850000] [Epoch 1400/4000] [D_F loss: 0.639726] [D_R loss: 0.713371] [G loss: -0.585071]\n",
      "[Lambda_f: 0.850000] [Epoch 1600/4000] [D_F loss: 0.620186] [D_R loss: 0.704989] [G loss: -0.566521]\n",
      "[Lambda_f: 0.850000] [Epoch 1800/4000] [D_F loss: 0.649018] [D_R loss: 0.712020] [G loss: -0.592910]\n",
      "[Lambda_f: 0.850000] [Epoch 2000/4000] [D_F loss: 0.622458] [D_R loss: 0.714482] [G loss: -0.569803]\n",
      "[Lambda_f: 0.850000] [Epoch 2200/4000] [D_F loss: 0.650976] [D_R loss: 0.702823] [G loss: -0.591082]\n",
      "[Lambda_f: 0.850000] [Epoch 2400/4000] [D_F loss: 0.626852] [D_R loss: 0.700545] [G loss: -0.570622]\n",
      "[Lambda_f: 0.850000] [Epoch 2600/4000] [D_F loss: 0.644751] [D_R loss: 0.705849] [G loss: -0.586399]\n",
      "[Lambda_f: 0.850000] [Epoch 2800/4000] [D_F loss: 0.632250] [D_R loss: 0.702308] [G loss: -0.574968]\n",
      "[Lambda_f: 0.850000] [Epoch 3000/4000] [D_F loss: 0.638572] [D_R loss: 0.695392] [G loss: -0.578869]\n",
      "[Lambda_f: 0.850000] [Epoch 3200/4000] [D_F loss: 0.637618] [D_R loss: 0.696733] [G loss: -0.578439]\n",
      "[Lambda_f: 0.850000] [Epoch 3400/4000] [D_F loss: 0.635706] [D_R loss: 0.698766] [G loss: -0.576855]\n",
      "[Lambda_f: 0.850000] [Epoch 3600/4000] [D_F loss: 0.640407] [D_R loss: 0.693102] [G loss: -0.579991]\n",
      "[Lambda_f: 0.850000] [Epoch 3800/4000] [D_F loss: 0.634538] [D_R loss: 0.691297] [G loss: -0.574659]\n",
      "Test accuracy: 0.6240000128746033\n",
      "P(y_hat=1 | z=0) = 0.426, P(y_hat=1 | z=1) = 0.225\n",
      "P(y_hat=1 | y=1, z=0) = 0.620, P(y_hat=1 | y=1, z=1) = 0.331\n",
      "Disparate Impact ratio = 0.528\n"
     ]
    }
   ],
   "source": [
    "train_result = []\n",
    "train_tensors = Namespace(XS_train = XS_train, y_train = y_train, s1_train = s1_train)\n",
    "val_tensors = Namespace(XS_val = XS_val, y_val = y_val, s1_val = s1_val) \n",
    "test_tensors = Namespace(XS_test = XS_test, y_test = y_test, s1_test = s1_test)\n",
    "\n",
    "train_opt = Namespace(val=len(y_val), n_epochs=4000, k=3, lr_g=0.005, lr_f=0.01, lr_r=0.001)      \n",
    "seed = 1 \n",
    "\n",
    "lambda_f_set = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85] # Lambda value for the fairness discriminator of FR-Train.\n",
    "lambda_r = 0.1 # Lambda value for the robustness discriminator of FR-Train.\n",
    "\n",
    "for lambda_f in lambda_f_set:\n",
    "    train_result.append(train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f = lambda_f, lambda_r = lambda_r, seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "------------------ Training Results of FR-Train on clean data ------------------\n",
      "[Lambda_f: 0.20] [Lambda_r: 0.10] Accuracy : 0.652, Disparate Impact : 0.938 \n",
      "[Lambda_f: 0.30] [Lambda_r: 0.10] Accuracy : 0.647, Disparate Impact : 0.850 \n",
      "[Lambda_f: 0.40] [Lambda_r: 0.10] Accuracy : 0.645, Disparate Impact : 0.816 \n",
      "[Lambda_f: 0.50] [Lambda_r: 0.10] Accuracy : 0.644, Disparate Impact : 0.787 \n",
      "[Lambda_f: 0.60] [Lambda_r: 0.10] Accuracy : 0.644, Disparate Impact : 0.776 \n",
      "[Lambda_f: 0.70] [Lambda_r: 0.10] Accuracy : 0.645, Disparate Impact : 0.781 \n",
      "[Lambda_f: 0.80] [Lambda_r: 0.10] Accuracy : 0.654, Disparate Impact : 0.955 \n",
      "[Lambda_f: 0.85] [Lambda_r: 0.10] Accuracy : 0.624, Disparate Impact : 0.528 \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"------------------ Training Results of FR-Train on clean data ------------------\" )\n",
    "for i in range(len(train_result)):\n",
    "    print(\n",
    "        \"[Lambda_f: %.2f] [Lambda_r: %.2f] Accuracy : %.3f, Disparate Impact : %.3f \"\n",
    "        % (train_result[i][0][0], train_result[i][0][1], train_result[i][0][2], train_result[i][0][3])\n",
    "    )       \n",
    "print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 7])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XS_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2103"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_0_mask = (s1 == 0.0)\n",
    "z_0 = int(torch.sum(z_0_mask))\n",
    "z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
